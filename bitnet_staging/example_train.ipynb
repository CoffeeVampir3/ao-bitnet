{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a68a307f-306d-4811-aa44-cebfc496548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import lovely_tensors as lt\n",
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "lt.monkey_patch()\n",
    "\n",
    "import os\n",
    "os.environ['TORCH_LOGS'] = \"output_code\"\n",
    "##https://github.com/fastai/imagenette\n",
    "\n",
    "## https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85438a53-cd29-4bae-86ca-cc593b39dbcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 10715\n",
      "Validation dataset size: 2679\n"
     ]
    }
   ],
   "source": [
    "# Set the URL and local path for the dataset\n",
    "url = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-160.tgz\"\n",
    "local_path = \"imagenette2-160.tgz\"\n",
    "\n",
    "# Download the dataset\n",
    "urllib.request.urlretrieve(url, local_path)\n",
    "\n",
    "# Extract the dataset\n",
    "with tarfile.open(local_path, \"r:gz\") as tar:\n",
    "    tar.extractall()\n",
    "\n",
    "# Remove the downloaded archive\n",
    "os.remove(local_path)\n",
    "\n",
    "# Set the path to the extracted dataset\n",
    "dataset_path = \"imagenette2-160\"\n",
    "\n",
    "# Define the transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((160, 160)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset using ImageFolder\n",
    "dataset = ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "# Split the dataset into train and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Print the sizes of the train and validation sets\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ab35d0d-1018-4f44-89e7-129cb47214f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageMLP, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = BitLinearTrain(160 * 160 * 3, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "class BitLinearTrain(nn.Linear):\n",
    "    def forward(self, x):\n",
    "        w = self.weight\n",
    "        x_norm = x\n",
    "        x_quant = x_norm + (activation_quant(x_norm) - x_norm).detach()\n",
    "        w_quant = w + (weight_quant(w) - w).detach()\n",
    "        y = F.linear(x_quant, w_quant)\n",
    "        return y\n",
    "\n",
    "def activation_quant(x):\n",
    "    scale = 127.0 / x.abs().max(dim=-1, keepdim=True).values.clamp_(min=1e-5)\n",
    "    y = (x * scale).round().clamp_(-128, 127) / scale\n",
    "    return y\n",
    "\n",
    "def weight_quant(w):\n",
    "    scale = 1.0 / w.abs().mean().clamp_(min=1e-5)\n",
    "    u = (w * scale).round().clamp_(-1, 1) / scale\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6485b81-023a-4160-9c1f-94c0cdc25fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[1, 10] x∈[0.033, 0.165] μ=0.100 σ=0.050 grad SoftmaxBackward0 [[0.066, 0.108, 0.157, 0.165, 0.143, 0.033, 0.041, 0.141, 0.090, 0.057]]\n"
     ]
    }
   ],
   "source": [
    "model = ImageMLP()\n",
    "image = torch.randn(1, 3, 160, 160)  # Batch size 1, single channel, 160x160 image\n",
    "output = model(image)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b05ae302-234a-4ac2-a469-d27dde130e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Train Loss: 1.9501, Train Acc: 0.5159, Val Loss: 1.9088, Val Acc: 0.5558\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to device\n",
    "model = ImageMLP().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate loss and accuracy\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_acc = train_correct / train_total\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Calculate loss and accuracy\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    val_acc = val_correct / val_total\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1bcbfe-c31e-4a39-87f8-f38b6bbc3cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: linear.weight\n",
      "Weight Shape: torch.Size([10, 76800])\n",
      "Weight Data: Parameter containing:\n",
      "Parameter[10, 76800] n=768000 (2.9Mb) x∈[-0.012, 0.011] μ=4.038e-05 σ=0.003 grad cuda:0\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Weight Shape: {param.shape}\")\n",
    "        print(f\"Weight Data: {param}\")\n",
    "        print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "079b04e0-aaaf-49fd-91bd-3b41ec78b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor \n",
    "\n",
    "def roundclip(x, a, b):\n",
    "    return torch.max(a, torch.min(b, torch.round(x)))\n",
    "\n",
    "def quantize_weights(weights):\n",
    "    # Compute the average absolute value of the weight matrix\n",
    "    gamma = torch.mean(torch.abs(weights))\n",
    "    \n",
    "    # Scale the weight matrix by the average absolute value\n",
    "    scaled_weights = weights / (gamma + 1e-8)\n",
    "    \n",
    "    # Round each scaled weight to the nearest integer in {-1, 0, +1}\n",
    "    quantized_weights = roundclip(scaled_weights, Tensor([-1]), Tensor([1]))\n",
    "    \n",
    "    return quantized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b67ab47-46f6-4160-a7e5-cc5f0b6d58f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model.linear.weight.detach().cpu()\n",
    "\n",
    "# Quantize the weights\n",
    "quantized_weights = quantize_weights(weights)\n",
    "\n",
    "# Assign the quantized weights back to the model\n",
    "model.linear.weight.data = quantized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5919be54-cde2-47a7-94f5-9dd167a12be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_weights = quantized_weights.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ca57dad-3c77-415e-a94c-b7e33ba2798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_size(size):\n",
    "    assert size[-1] % 4 == 0, f\"{size} last dim not divisible by four\"\n",
    "    return (*size[:-1], size[-1] // 4)\n",
    "\n",
    "def up_size(size):\n",
    "    return (*size[:-1], size[-1] * 4)\n",
    "\n",
    "#unpack int8\n",
    "@torch.compile\n",
    "def unpack_uint8_to_trinary2(uint8_data) -> torch.Tensor:\n",
    "    \"\"\"Get the original weight from the normalized float weight format\"\"\"\n",
    "    # since we are using uint8 we will decode 4 entries per byte\n",
    "    shape = uint8_data.shape\n",
    "    first_elements = ((uint8_data >> 6) & 0b11).to(torch.int8) - 1\n",
    "    second_elements = ((uint8_data >> 4) & 0b11).to(torch.int8) - 1\n",
    "    third_elements = ((uint8_data >> 2) & 0b11).to(torch.int8) - 1\n",
    "    fourth_elements = (uint8_data & 0b11).to(torch.int8) - 1\n",
    "    return torch.stack([first_elements, second_elements, third_elements, fourth_elements], dim=-1).view(up_size(shape))\n",
    "\n",
    "#packing uint8\n",
    "@torch.compile\n",
    "def pack_int2(uint8_data) -> torch.Tensor:\n",
    "    # converting to uint8 for operations\n",
    "    shape = uint8_data.shape\n",
    "    assert shape[-1] % 4 == 0\n",
    "    uint8_data = uint8_data.contiguous().view(-1)\n",
    "    packed_data = (uint8_data[::4] << 6 | uint8_data[1::4] << 4 | uint8_data[2::4] << 2 | uint8_data[3::4]).view(down_size(shape))\n",
    "    return packed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeed3e96-e1c4-45e2-a351-f3a6785868e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 76800] u8 n=768000 (0.7Mb) x∈[0, 2] μ=1.012 σ=0.845 cuda:0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shifted_layer = (quantized_weights + 1.0).to(torch.uint8).to(device)\n",
    "shifted_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9442288b-9535-4846-9a18-107d8a65254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_int2(shifted_layer).to(device)\n",
    "unpacked = unpack_uint8_to_trinary2(packed).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0db93b0-bbde-4307-b7b9-ade879d83f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor[10, 76800] i8 n=768000 (0.7Mb) x∈[-1, 1] μ=0.012 σ=0.845 cuda:0\n",
      "torch.int8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(unpacked)\n",
    "print(unpacked.dtype)\n",
    "print(unpacked.allclose(quantized_weights.to(torch.int8)))\n",
    "assert(unpacked.allclose(quantized_weights.to(torch.int8)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
